apiVersion: v1
kind: Service
metadata:
  name: inference-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30000
  selector:
    app: inference-service

---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: inference-server
  labels:
    kueue.x-k8s.io/queue-name: multislice-queue
spec:
  replicatedJobs:
    - name: inference
      template:
        spec:
          parallelism: 8
          completions: 8
          template:
            metadata:
              labels:
                app: inference-service
            spec:
              restartPolicy: OnFailure
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: gpu.amd.com/gpu-${JOB_COMPLETION_INDEX}
                        operator: Exists
                podAntiAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchLabels:
                        app: inference-service
                    topologyKey: "kubernetes.io/hostname"
              containers:
              - name: inference
                image: inference-image:latest
                env:
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: JOB_COMPLETION_INDEX
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                - name: ROCR_VISIBLE_DEVICES
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                volumeMounts:
                - name: model-weights
                  mountPath: /models
                resources:
                  limits:
                    amd.com/gpu: 1
                    memory: "180Gi"
                  requests:
                    amd.com/gpu: 1
                    memory: "170Gi"
              volumes:
              - name: model-weights
                persistentVolumeClaim:
                  claimName: model-weights-${JOB_COMPLETION_INDEX}

---
# PVC Template
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-weights-${JOB_COMPLETION_INDEX}
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 200Gi

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
