apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-model-weights  # Single PVC for model weights
  annotations:
    volume.beta.kubernetes.io/storage-class: "local-storage"
spec:
  accessModes:
    - ReadOnlyMany  # Allow multiple pods to read
  resources:
    requests:
      storage: 200Gi  # Adjust based on model size

---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: vllm-inference
  labels:
    kueue.x-k8s.io/queue-name: multislice-queue
    xpk.google.com/workload: vllm-inference
spec:
  failurePolicy:
    maxRestarts: 0
  replicatedJobs:
    - name: slice-job
      replicas: 1
      template:
        spec:
          parallelism: 1
          completions: 1
          backoffLimit: 0
          template:
            metadata:
              labels:
                xpk.google.com/workload: vllm-inference
                essential.ai/base-workload: vllm
            spec:
              schedulerName: default-scheduler
              restartPolicy: Never
              priorityClassName: medium
              hostNetwork: true
              hostIPC: true
              dnsPolicy: ClusterFirstWithHostNet
              volumes:
              - name: dev-kfd
                hostPath:
                  path: /dev/kfd
              - name: dev-dri
                hostPath:
                  path: /dev/dri
              - name: shared-memory
                emptyDir:
                  medium: "Memory"
                  sizeLimit: 200Gi
              - name: model-weights
                persistentVolumeClaim:
                  claimName: vllm-model-weights
                  readOnly: true
              containers:
              - name: vllm-inference
                image: gcr.io/minerva-394000/vllm-dev:20241121-tuned
                env:
                  - name: REPLICATED_JOB_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                  - name: JOBSET_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
                  - name: NODE_RANK
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
                  - name: GPUS_PER_NODE
                    value: "8"
                securityContext:
                  privileged: true
                  capabilities:
                    add: ["SYS_PTRACE"]
                command:
                - bash
                - -c
                - |
                  echo "Starting vLLM inference service..."
                  export PYTHONPATH=""
                  export ROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
                  export HSA_FORCE_FINE_GRAIN_PCIE=1
                  export NCCL_SOCKET_IFNAME=bond0.2142
                  
                  vllm serve /models/Llama-3.2-8B-Instruct/ \
                    --tensor-parallel-size=8 \
                    --max-num-batched-tokens=8192
                resources:
                  limits:
                    amd.com/gpu: 8
                volumeMounts:
                  - name: shared-memory
                    mountPath: /dev/shm
                  - name: dev-kfd
                    mountPath: /dev/kfd
                  - name: dev-dri
                    mountPath: /dev/dri
                  - name: model-weights
                    mountPath: /models
                    readOnly: true

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
